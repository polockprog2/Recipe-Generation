{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":5245331,"sourceType":"datasetVersion","datasetId":3052101}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================\n# TPU VM-SAFE MULTI-MODEL SEQ2SEQ TRAINING WITH ACCUMULATION & BF16\n# ============================\n\nimport os, gc, math, ast, re, warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\n# TPU\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler\nfrom torch.optim import AdamW\n\n# Metrics\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.gleu_score import sentence_gleu\nimport nltk\nnltk.download('punkt', quiet=True)\n\n# ---------------------------\n# CONFIGURATION\n# ---------------------------\nDATA_PATH = \"/kaggle/input/recipenlg/dataset/full_dataset.csv\"\nMODELS_TO_TRAIN = [\"t5-small\", \"t5-base\", \"facebook/bart-base\", \"sshleifer/distilbart-cnn-12-6\"]\nFRAC = 0.10\nMAX_IN = 64\nMAX_OUT = 12\nBATCH_SIZE = 4\nACCUM = 2         # gradient accumulation steps\nEPOCHS = 3\nLR = 3e-4\nSAVE_DIR = \"./best_model_multi\"\n\ndevice = xm.xla_device()  # TPU core\n\ndef free_mem():\n    gc.collect()\n    try: torch.cuda.empty_cache()\n    except: pass\n\n# ---------------------------\n# LOAD & CLEAN DATA\n# ---------------------------\ndf = pd.read_csv(DATA_PATH)\ndf.columns = [c.lower().strip() for c in df.columns]\ndf = df.dropna(subset=[\"title\",\"ner\"])\n\ndef parse_ner(x):\n    try:\n        arr = ast.literal_eval(x)\n        arr = [str(a).lower().strip() for a in arr]\n        return \", \".join(sorted(list(set(arr))))\n    except:\n        return str(x).lower()\n\ndef clean_text(t):\n    t = str(t).lower()\n    t = re.sub(r\"[^a-z0-9 ]\", \" \", t)\n    return re.sub(r\"\\s+\", \" \", t).strip()\n\ndf[\"ingredients\"] = df[\"ner\"].apply(parse_ner)\ndf[\"title\"] = df[\"title\"].apply(clean_text)\ndf = df[df[\"title\"].str.len() > 5]\ndf = df[df[\"ingredients\"].str.len() > 5]\ndf = df.sample(frac=FRAC, random_state=42).reset_index(drop=True)\n\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n\n# ---------------------------\n# DATASET CLASS\n# ---------------------------\nclass RecipeDS(Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df\n        self.tok = tokenizer\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        src = \"generate title: \" + row[\"ingredients\"]\n        tgt = row[\"title\"]\n\n        x = self.tok(src, max_length=MAX_IN, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n        y = self.tok(tgt, max_length=MAX_OUT, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n\n        labels = y[\"input_ids\"].squeeze()\n        labels[labels == self.tok.pad_token_id] = -100\n\n        return {\n            \"input_ids\": x[\"input_ids\"].squeeze(),\n            \"attention_mask\": x[\"attention_mask\"].squeeze(),\n            \"labels\": labels\n        }\n\n# ---------------------------\n# METRICS\n# ---------------------------\nscorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\nsmooth_fn = SmoothingFunction().method1\n\ndef compute_metrics(preds, refs):\n    rouge_l = np.mean([scorer.score(p,r)[\"rougeL\"].fmeasure for p,r in zip(preds, refs)])*100\n    bleu = np.mean([sentence_bleu([r.split()], p.split(), smoothing_function=smooth_fn) for p,r in zip(preds, refs)])*100\n    gleu = np.mean([sentence_gleu([r.split()], p.split()) for p,r in zip(preds, refs)])*100\n    return {\"rougeL\": rouge_l, \"bleu\": bleu, \"gleu\": gleu}\n\n# ---------------------------\n# TRAINING FUNCTION\n# ---------------------------\ndef train_model(model_name):\n    print(f\"\\n=== TRAINING {model_name} ===\")\n    tok = AutoTokenizer.from_pretrained(model_name)\n    if tok.pad_token is None:\n        tok.add_special_tokens({\"pad_token\":\"[PAD]\"})\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.resize_token_embeddings(len(tok))\n    model = model.to(dtype=torch.bfloat16)  # TPU bf16\n\n    train_ds = RecipeDS(train_df, tok)\n    val_ds = RecipeDS(val_df, tok)\n    test_ds = RecipeDS(test_df, tok)\n\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n\n    optim = AdamW(model.parameters(), lr=LR)\n    steps = math.ceil(len(train_ds)/BATCH_SIZE/ACCUM)*EPOCHS\n    sched = get_scheduler(\"linear\", optimizer=optim, num_warmup_steps=0, num_training_steps=steps)\n\n    best_score = -1\n\n    for ep in range(EPOCHS):\n        model.train()\n        running_loss = 0\n        optim.zero_grad()\n        for i, batch in enumerate(train_loader):\n            batch = {k:v.to(device).to(dtype=torch.bfloat16) if k!=\"labels\" else v.to(device) for k,v in batch.items()}\n            out = model(**batch)\n            loss = out.loss / ACCUM\n            loss.backward()\n            running_loss += out.loss.detach().cpu().item()\n\n            if (i+1) % ACCUM == 0:\n                optim.step()\n                sched.step()\n                optim.zero_grad()\n                xm.mark_step()\n\n            if (i+1) % (20*ACCUM) == 0:\n                print(f\"Epoch {ep+1} Step {i+1} Loss {running_loss/(i+1):.4f}\")\n\n        # Validation\n        model.eval()\n        preds, refs = [], []\n        for b in val_loader:\n            with torch.no_grad():\n                b = {k:v.to(device) for k,v in b.items()}\n                gen = model.generate(\n                    b[\"input_ids\"],\n                    attention_mask=b[\"attention_mask\"],\n                    max_length=MAX_OUT+8,\n                    num_beams=4\n                )\n            p = tok.batch_decode(gen.cpu(), skip_special_tokens=True)\n            lab = b[\"labels\"].cpu().numpy()\n            lab[lab==-100]=tok.pad_token_id\n            r = tok.batch_decode(lab, skip_special_tokens=True)\n            preds += p\n            refs += r\n\n        scores = compute_metrics(preds, refs)\n        print(f\"Epoch {ep+1} | Val Scores: ROUGE-L={scores['rougeL']:.2f}, BLEU={scores['bleu']:.2f}, GLEU={scores['gleu']:.2f}\")\n\n        # Save best model\n        if scores['rougeL'] > best_score:\n            best_score = scores['rougeL']\n            os.makedirs(os.path.join(SAVE_DIR, model_name.replace(\"/\",\"_\")), exist_ok=True)\n            model.save_pretrained(os.path.join(SAVE_DIR, model_name.replace(\"/\",\"_\")))\n            tok.save_pretrained(os.path.join(SAVE_DIR, model_name.replace(\"/\",\"_\")))\n            print(\">>> Best model saved.\")\n\n        free_mem()\n\n    # Test evaluation\n    model.eval()\n    preds, refs = [], []\n    for b in test_loader:\n        with torch.no_grad():\n            b = {k:v.to(device) for k,v in b.items()}\n            gen = model.generate(\n                b[\"input_ids\"],\n                attention_mask=b[\"attention_mask\"],\n                max_length=MAX_OUT+8,\n                num_beams=4\n            )\n        p = tok.batch_decode(gen.cpu(), skip_special_tokens=True)\n        lab = b[\"labels\"].cpu().numpy()\n        lab[lab==-100]=tok.pad_token_id\n        r = tok.batch_decode(lab, skip_special_tokens=True)\n        preds += p\n        refs += r\n    test_scores = compute_metrics(preds, refs)\n    print(f\"TEST SCORES {model_name}: ROUGE-L={test_scores['rougeL']:.2f}, BLEU={test_scores['bleu']:.2f}, GLEU={test_scores['gleu']:.2f}\")\n\n    free_mem()\n    return test_scores\n\n# ---------------------------\n# RUN ALL MODELS\n# ---------------------------\nall_results = {}\nfor m in MODELS_TO_TRAIN:\n    try:\n        res = train_model(m)\n        all_results[m] = res\n    except Exception as e:\n        print(f\"Model {m} failed: {e}\")\n        all_results[m] = {\"error\": str(e)}\n\nprint(\"\\n=== ALL MODELS FINISHED ===\")\nprint(all_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:54:41.503869Z","iopub.execute_input":"2025-11-26T08:54:41.504175Z"}},"outputs":[],"execution_count":null}]}
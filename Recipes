{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5245331,"sourceType":"datasetVersion","datasetId":3052101}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================\n# GPU-OPTIMIZED SEQ2SEQ TRAINING\n# ============================\n\nimport os, gc, math, ast, re, warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom torch.optim import AdamW\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    get_scheduler\n)\nfrom torch.optim import AdamW \nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.gleu_score import sentence_gleu\nfrom sklearn.model_selection import train_test_split\n\n# ---------------------------\n# CONFIG\n# ---------------------------\nDATA_PATH = \"/kaggle/input/recipenlg/dataset/full_dataset.csv\"\nMODELS = [\n    \"t5-small\", \"t5-base\", \"facebook/bart-base\", \n    \"sshleifer/distilbart-cnn-12-6\"\n]\n\nFRAC = 0.1\nMAX_IN = 64\nMAX_OUT = 12\n\nBATCH_SIZE = 4\nACCUM = 2\nEPOCHS = 3\nLR = 3e-4\n\nSAVE_DIR = \"./best_model\"\n\n# ---------------------------\n# UTILITY\n# ---------------------------\ndef free():\n    gc.collect()\n    try: torch.cuda.empty_cache()\n    except: pass\n\n# ---------------------------\n# LOAD + CLEAN DATA\n# ---------------------------\ndf = pd.read_csv(DATA_PATH)\ndf.columns = [c.lower().strip() for c in df.columns]\ndf = df.dropna(subset=[\"title\", \"ner\"])\n\ndef parse(x):\n    try:\n        arr = ast.literal_eval(x)\n        arr = [str(a).lower().strip() for a in arr]\n        return \", \".join(sorted(list(set(arr))))\n    except:\n        return str(x).lower()\n\ndef clean(t):\n    t = str(t).lower()\n    t = re.sub(r\"[^a-z0-9 ]\", \" \", t)\n    return re.sub(r\"\\s+\", \" \", t).strip()\n\ndf[\"ingredients\"] = df[\"ner\"].apply(parse)\ndf[\"title\"] = df[\"title\"].apply(clean)\n\ndf = df[df[\"title\"].str.len() > 5]\ndf = df[df[\"ingredients\"].str.len() > 5]\ndf = df.sample(frac=FRAC, random_state=42).reset_index(drop=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=1)\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=1)\n\n# ---------------------------\n# DATASET\n# ---------------------------\nclass RecipeDS(Dataset):\n    def __init__(self, df, tokenizer, max_in=64, max_out=12):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_in = max_in\n        self.max_out = max_out\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        src = \"generate title: \" + row[\"ingredients\"]\n        tgt = row[\"title\"]\n\n        x = self.tokenizer(src, max_length=self.max_in, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n        y = self.tokenizer(tgt, max_length=self.max_out, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n\n        labels = y[\"input_ids\"].squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": x[\"input_ids\"].squeeze(),\n            \"attention_mask\": x[\"attention_mask\"].squeeze(),\n            \"labels\": labels\n        }\n\n# ---------------------------\n# METRICS\n# ---------------------------\nscorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\ndef rougeL(preds, refs):\n    scores = [scorer.score(a, b)[\"rougeL\"].fmeasure for a, b in zip(preds, refs)]\n    return float(np.mean(scores)) * 100\n\ndef bleu_score(preds, refs):\n    return np.mean([sentence_bleu([r.split()], p.split(), weights=(0.5,0.5)) for p,r in zip(preds, refs)])*100\n\ndef gleu_score(preds, refs):\n    return np.mean([sentence_gleu([r.split()], p.split()) for p,r in zip(preds, refs)])*100\n\n# ---------------------------\n# DEVICE\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ---------------------------\n# TRAINING LOOP\n# ---------------------------\nresults = {}\n\nfor model_name in MODELS:\n    try:\n        print(f\"\\n=== TRAINING {model_name} ===\")\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if tokenizer.pad_token is None:\n            tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n        model.resize_token_embeddings(len(tokenizer))\n\n        train_ds = RecipeDS(train_df, tokenizer, MAX_IN, MAX_OUT)\n        val_ds = RecipeDS(val_df, tokenizer, MAX_IN, MAX_OUT)\n        test_ds = RecipeDS(test_df, tokenizer, MAX_IN, MAX_OUT)\n\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n\n        optim = AdamW(model.parameters(), lr=LR)\n        steps = math.ceil(len(train_ds) / (BATCH_SIZE * ACCUM)) * EPOCHS\n        sched = get_scheduler(\"linear\", optimizer=optim, num_warmup_steps=0, num_training_steps=steps)\n\n        scaler = torch.cuda.amp.GradScaler()\n        best_score = -1\n        global_step = 0\n\n        for ep in range(EPOCHS):\n            model.train()\n            running_loss = 0\n\n            for i, batch in enumerate(train_loader):\n                batch = {k:v.to(device) for k,v in batch.items()}\n\n                with torch.cuda.amp.autocast():\n                    out = model(**batch)\n                    loss = out.loss / ACCUM\n\n                scaler.scale(loss).backward()\n                running_loss += out.loss.detach().cpu().item()\n\n                if (i+1) % ACCUM == 0:\n                    scaler.step(optim)\n                    scaler.update()\n                    sched.step()\n                    optim.zero_grad()\n                    global_step += 1\n\n                if (i+1) % (20*ACCUM) == 0:\n                    print(f\"Epoch {ep+1} Step {i+1} Loss {running_loss/(i+1):.4f}\")\n\n            # Validation\n            model.eval()\n            preds, refs = [], []\n            with torch.no_grad():\n                for b in val_loader:\n                    b = {k:v.to(device) for k,v in b.items()}\n                    gen = model.generate(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"],\n                                         max_length=MAX_OUT+8, num_beams=4)\n                    p = tokenizer.batch_decode(gen, skip_special_tokens=True)\n                    lab = b[\"labels\"].cpu().numpy()\n                    lab[lab==-100] = tokenizer.pad_token_id\n                    r = tokenizer.batch_decode(lab, skip_special_tokens=True)\n                    preds += p\n                    refs += r\n\n            score = rougeL(preds, refs)\n            print(f\"Epoch {ep+1} | Val ROUGE-L = {score:.2f}\")\n\n            if score > best_score:\n                best_score = score\n                os.makedirs(os.path.join(SAVE_DIR, model_name.replace(\"/\", \"_\")), exist_ok=True)\n                model.save_pretrained(os.path.join(SAVE_DIR, model_name.replace(\"/\", \"_\")))\n                tokenizer.save_pretrained(os.path.join(SAVE_DIR, model_name.replace(\"/\", \"_\")))\n                print(\">>> Best model saved.\")\n\n        # Test evaluation\n        model.eval()\n        preds, refs = [], []\n        with torch.no_grad():\n            for b in test_loader:\n                b = {k:v.to(device) for k,v in b.items()}\n                gen = model.generate(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"],\n                                     max_length=MAX_OUT+8, num_beams=4)\n                p = tokenizer.batch_decode(gen, skip_special_tokens=True)\n                lab = b[\"labels\"].cpu().numpy()\n                lab[lab==-100] = tokenizer.pad_token_id\n                r = tokenizer.batch_decode(lab, skip_special_tokens=True)\n                preds += p\n                refs += r\n\n        results[model_name] = {\n            \"rougeL\": rougeL(preds, refs),\n            \"bleu\": bleu_score(preds, refs),\n            \"gleu\": gleu_score(preds, refs)\n        }\n        print(f\"=== {model_name} TEST SCORES ===\", results[model_name])\n\n        free()\n\n    except Exception as e:\n        results[model_name] = {\"error\": str(e)}\n        print(f\"Model {model_name} failed:\", e)\n\nprint(\"\\n=== ALL MODELS FINISHED ===\")\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T09:14:01.406236Z","iopub.execute_input":"2025-11-26T09:14:01.406906Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n=== TRAINING t5-small ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec6e57967c9415484b58feb9af1188b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"721b69c971514a948ab2709941f16334"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9ba757a6d749978dc9ea4d216cd5e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbdbd90043544bc4b6f39d68c35b700d"}},"metadata":{}},{"name":"stderr","text":"2025-11-26 09:15:42.009923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764148542.197007      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764148542.244580      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a637ce6a517c44a79645cb6ec9b31b0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57c08bd1401445ba2274e9a60f72f51"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Step 40 Loss 5.4211\nEpoch 1 Step 80 Loss 4.7474\nEpoch 1 Step 120 Loss 4.4945\nEpoch 1 Step 160 Loss 4.3086\nEpoch 1 Step 200 Loss 4.2046\nEpoch 1 Step 240 Loss 4.1667\nEpoch 1 Step 280 Loss 4.1048\nEpoch 1 Step 320 Loss 4.0664\nEpoch 1 Step 360 Loss 4.0344\nEpoch 1 Step 400 Loss 4.0078\nEpoch 1 Step 440 Loss 3.9916\nEpoch 1 Step 480 Loss 3.9759\nEpoch 1 Step 520 Loss 3.9581\nEpoch 1 Step 560 Loss 3.9418\nEpoch 1 Step 600 Loss 3.9128\nEpoch 1 Step 640 Loss 3.9008\nEpoch 1 Step 680 Loss 3.8871\nEpoch 1 Step 720 Loss 3.8710\nEpoch 1 Step 760 Loss 3.8579\nEpoch 1 Step 800 Loss 3.8442\nEpoch 1 Step 840 Loss 3.8272\nEpoch 1 Step 880 Loss 3.8116\nEpoch 1 Step 920 Loss 3.7956\nEpoch 1 Step 960 Loss 3.7890\nEpoch 1 Step 1000 Loss 3.7750\nEpoch 1 Step 1040 Loss 3.7679\nEpoch 1 Step 1080 Loss 3.7614\nEpoch 1 Step 1120 Loss 3.7583\nEpoch 1 Step 1160 Loss 3.7428\nEpoch 1 Step 1200 Loss 3.7327\nEpoch 1 Step 1240 Loss 3.7251\nEpoch 1 Step 1280 Loss 3.7188\nEpoch 1 Step 1320 Loss 3.7140\nEpoch 1 Step 1360 Loss 3.7045\nEpoch 1 Step 1400 Loss 3.6945\nEpoch 1 Step 1440 Loss 3.6864\nEpoch 1 Step 1480 Loss 3.6734\nEpoch 1 Step 1520 Loss 3.6717\nEpoch 1 Step 1560 Loss 3.6676\nEpoch 1 Step 1600 Loss 3.6630\nEpoch 1 Step 1640 Loss 3.6573\nEpoch 1 Step 1680 Loss 3.6536\nEpoch 1 Step 1720 Loss 3.6496\nEpoch 1 Step 1760 Loss 3.6460\nEpoch 1 Step 1800 Loss 3.6430\nEpoch 1 Step 1840 Loss 3.6429\nEpoch 1 Step 1880 Loss 3.6358\nEpoch 1 Step 1920 Loss 3.6316\nEpoch 1 Step 1960 Loss 3.6283\nEpoch 1 Step 2000 Loss 3.6228\nEpoch 1 Step 2040 Loss 3.6119\nEpoch 1 Step 2080 Loss 3.6040\nEpoch 1 Step 2120 Loss 3.6036\nEpoch 1 Step 2160 Loss 3.5955\nEpoch 1 Step 2200 Loss 3.5922\nEpoch 1 Step 2240 Loss 3.5864\nEpoch 1 Step 2280 Loss 3.5814\nEpoch 1 Step 2320 Loss 3.5786\nEpoch 1 Step 2360 Loss 3.5732\nEpoch 1 Step 2400 Loss 3.5663\nEpoch 1 Step 2440 Loss 3.5629\nEpoch 1 Step 2480 Loss 3.5602\nEpoch 1 Step 2520 Loss 3.5566\nEpoch 1 Step 2560 Loss 3.5534\nEpoch 1 Step 2600 Loss 3.5489\nEpoch 1 Step 2640 Loss 3.5406\nEpoch 1 Step 2680 Loss 3.5382\nEpoch 1 Step 2720 Loss 3.5345\nEpoch 1 Step 2760 Loss 3.5317\nEpoch 1 Step 2800 Loss 3.5272\nEpoch 1 Step 2840 Loss 3.5242\nEpoch 1 Step 2880 Loss 3.5203\nEpoch 1 Step 2920 Loss 3.5166\nEpoch 1 Step 2960 Loss 3.5124\nEpoch 1 Step 3000 Loss 3.5073\nEpoch 1 Step 3040 Loss 3.5065\nEpoch 1 Step 3080 Loss 3.5028\nEpoch 1 Step 3120 Loss 3.4994\nEpoch 1 Step 3160 Loss 3.4975\nEpoch 1 Step 3200 Loss 3.4953\nEpoch 1 Step 3240 Loss 3.4923\nEpoch 1 Step 3280 Loss 3.4901\nEpoch 1 Step 3320 Loss 3.4859\nEpoch 1 Step 3360 Loss 3.4844\nEpoch 1 Step 3400 Loss 3.4801\nEpoch 1 Step 3440 Loss 3.4774\nEpoch 1 Step 3480 Loss 3.4742\nEpoch 1 Step 3520 Loss 3.4728\nEpoch 1 Step 3560 Loss 3.4678\nEpoch 1 Step 3600 Loss 3.4650\nEpoch 1 Step 3640 Loss 3.4625\nEpoch 1 Step 3680 Loss 3.4584\nEpoch 1 Step 3720 Loss 3.4575\nEpoch 1 Step 3760 Loss 3.4532\nEpoch 1 Step 3800 Loss 3.4521\nEpoch 1 Step 3840 Loss 3.4493\nEpoch 1 Step 3880 Loss 3.4464\nEpoch 1 Step 3920 Loss 3.4457\nEpoch 1 Step 3960 Loss 3.4442\nEpoch 1 Step 4000 Loss 3.4427\nEpoch 1 Step 4040 Loss 3.4385\nEpoch 1 Step 4080 Loss 3.4363\nEpoch 1 Step 4120 Loss 3.4328\nEpoch 1 Step 4160 Loss 3.4327\nEpoch 1 Step 4200 Loss 3.4307\nEpoch 1 Step 4240 Loss 3.4285\nEpoch 1 Step 4280 Loss 3.4240\nEpoch 1 Step 4320 Loss 3.4231\nEpoch 1 Step 4360 Loss 3.4203\nEpoch 1 Step 4400 Loss 3.4171\nEpoch 1 Step 4440 Loss 3.4165\nEpoch 1 Step 4480 Loss 3.4131\nEpoch 1 Step 4520 Loss 3.4104\nEpoch 1 Step 4560 Loss 3.4093\nEpoch 1 Step 4600 Loss 3.4071\nEpoch 1 Step 4640 Loss 3.4046\nEpoch 1 Step 4680 Loss 3.4018\nEpoch 1 Step 4720 Loss 3.4002\nEpoch 1 Step 4760 Loss 3.3980\nEpoch 1 Step 4800 Loss 3.3972\nEpoch 1 Step 4840 Loss 3.3960\nEpoch 1 Step 4880 Loss 3.3935\nEpoch 1 Step 4920 Loss 3.3904\nEpoch 1 Step 4960 Loss 3.3861\nEpoch 1 Step 5000 Loss 3.3833\nEpoch 1 Step 5040 Loss 3.3809\nEpoch 1 Step 5080 Loss 3.3784\nEpoch 1 Step 5120 Loss 3.3782\nEpoch 1 Step 5160 Loss 3.3764\nEpoch 1 Step 5200 Loss 3.3749\nEpoch 1 Step 5240 Loss 3.3739\nEpoch 1 Step 5280 Loss 3.3720\nEpoch 1 Step 5320 Loss 3.3704\nEpoch 1 Step 5360 Loss 3.3679\nEpoch 1 Step 5400 Loss 3.3659\nEpoch 1 Step 5440 Loss 3.3639\nEpoch 1 Step 5480 Loss 3.3621\nEpoch 1 Step 5520 Loss 3.3605\nEpoch 1 Step 5560 Loss 3.3584\nEpoch 1 Step 5600 Loss 3.3567\nEpoch 1 Step 5640 Loss 3.3563\nEpoch 1 Step 5680 Loss 3.3549\nEpoch 1 Step 5720 Loss 3.3527\nEpoch 1 Step 5760 Loss 3.3509\nEpoch 1 Step 5800 Loss 3.3499\nEpoch 1 Step 5840 Loss 3.3485\nEpoch 1 Step 5880 Loss 3.3465\nEpoch 1 Step 5920 Loss 3.3447\nEpoch 1 Step 5960 Loss 3.3424\nEpoch 1 Step 6000 Loss 3.3397\nEpoch 1 Step 6040 Loss 3.3384\nEpoch 1 Step 6080 Loss 3.3362\nEpoch 1 Step 6120 Loss 3.3337\nEpoch 1 Step 6160 Loss 3.3323\nEpoch 1 Step 6200 Loss 3.3309\nEpoch 1 Step 6240 Loss 3.3283\nEpoch 1 Step 6280 Loss 3.3263\nEpoch 1 Step 6320 Loss 3.3247\nEpoch 1 Step 6360 Loss 3.3230\nEpoch 1 Step 6400 Loss 3.3223\nEpoch 1 Step 6440 Loss 3.3205\nEpoch 1 Step 6480 Loss 3.3197\nEpoch 1 Step 6520 Loss 3.3181\nEpoch 1 Step 6560 Loss 3.3162\nEpoch 1 Step 6600 Loss 3.3146\nEpoch 1 Step 6640 Loss 3.3135\nEpoch 1 Step 6680 Loss 3.3124\nEpoch 1 Step 6720 Loss 3.3111\nEpoch 1 Step 6760 Loss 3.3095\nEpoch 1 Step 6800 Loss 3.3081\nEpoch 1 Step 6840 Loss 3.3060\nEpoch 1 Step 6880 Loss 3.3035\nEpoch 1 Step 6920 Loss 3.3017\nEpoch 1 Step 6960 Loss 3.3002\nEpoch 1 Step 7000 Loss 3.2990\nEpoch 1 Step 7040 Loss 3.2973\nEpoch 1 Step 7080 Loss 3.2955\nEpoch 1 Step 7120 Loss 3.2942\nEpoch 1 Step 7160 Loss 3.2923\nEpoch 1 Step 7200 Loss 3.2906\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
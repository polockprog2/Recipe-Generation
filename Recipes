
import os, gc, math, ast, re, warnings
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler
import pandas as pd
import numpy as np
import tqdm
from sklearn.model_selection import train_test_split

from rouge_score import rouge_scorer
from nltk.translate.bleu_score import corpus_bleu 
from nltk.translate.gleu_score import corpus_gleu 

# ---------------------------
# CONFIG
# ---------------------------
DATA_PATH = "/kaggle/input/recipenlg/dataset/full_dataset.csv"
MODELS = [
    "t5-small", "t5-base", "facebook/bart-base","sshleifer/distilbart-cnn-12-6"
]

FRAC = 0.09           
MAX_IN = 128            
MAX_OUT = 24            

BATCH_SIZE = 8          
ACCUM = 2               
EPOCHS = 3
LR = 3e-4

SAVE_DIR = "./best_model"

# ---------------------------
# UTILITY
# ---------------------------
def free():
    gc.collect()
    try: torch.cuda.empty_cache()
    except: pass

# ---------------------------
# LOAD + CLEAN DATA + FEATURE ENGINEERING
# ---------------------------
df = pd.read_csv(DATA_PATH)
df.columns = [c.lower().strip() for c in df.columns]
df = df.dropna(subset=["title", "ner"])

# --- FEATURE ENGINEERING: Define low-information words to remove ---
LOW_INFO_WORDS = set([
    'ounce', 'oz', 'cup', 'tsp', 'tbsp', 'teaspoon', 'tablespoon', 
    'pound', 'pkg', 'can', 'g', 'ml', 'gram', 'milliliter', 'cloves',
    'fresh', 'dried', 'large', 'small', 'medium', 'chopped', 'sliced', 
    'diced', 'minced', 'about', 'some', 'to', 'for', 'of', 'and'
])
# -----------------------------------------------------------------

def parse(x):
    try:
        arr = ast.literal_eval(x)
        arr = [str(a).lower().strip() for a in arr]
    except:
        arr = [str(x).lower().strip()]
    
    # 1. Clean and tokenize
    cleaned_arr = []
    for ingredient in arr:
        tokens = re.findall(r'\b\w+\b', ingredient)
        
        # 2. Prune low-info words
        meaningful_tokens = [t for t in tokens if t not in LOW_INFO_WORDS and len(t) > 1]
        
        # Reconstruct the ingredient phrase
        if meaningful_tokens:
            cleaned_arr.append(" ".join(meaningful_tokens))

    # Remove duplicates and join
    return ", ".join(sorted(list(set(cleaned_arr))))

def clean(t):
    """Cleans the recipe title (output)."""
    t = str(t).lower()
    t = re.sub(r"[^a-z0-9 ]", " ", t)
    return re.sub(r"\s+", " ", t).strip()

df["ingredients"] = df["ner"].apply(parse)
df["title"] = df["title"].apply(clean)

df = df[df["title"].str.len() > 5]
df = df[df["ingredients"].str.len() > 5]
df = df.sample(frac=FRAC, random_state=42).reset_index(drop=True)

train_df, test_df = train_test_split(df, test_size=0.1, random_state=1)
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=1)

# ---------------------------
# DATASET
# ---------------------------
class RecipeDS(Dataset):
    def __init__(self, df, tokenizer, max_in=128, max_out=24):
        self.df = df
        self.tokenizer = tokenizer
        self.max_in = max_in
        self.max_out = max_out

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        # FE: Apply T5 Task Prefix for consistent model behavior
        src = "generate title: " + row["ingredients"] 
        
        tgt = row["title"]

        x = self.tokenizer(src, max_length=self.max_in, truncation=True, padding="max_length", return_tensors="pt")
        y = self.tokenizer(tgt, max_length=self.max_out, truncation=True, padding="max_length", return_tensors="pt")

        labels = y["input_ids"].squeeze()
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            "input_ids": x["input_ids"].squeeze(),
            "attention_mask": x["attention_mask"].squeeze(),
            "labels": labels
        }

# ---------------------------
# METRICS 
# ---------------------------
scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
def rougeL(preds, refs):
    scores = [scorer.score(a, b)["rougeL"].fmeasure for a, b in zip(preds, refs)]
    return float(np.mean(scores)) * 100

def bleu_score(preds, refs):
    refs_tok = [[r.split()] for r in refs]
    preds_tok = [p.split() for p in preds]
    return corpus_bleu(refs_tok, preds_tok) * 100

def gleu_score(preds, refs):
    refs_tok = [[r.split()] for r in refs]
    preds_tok = [p.split() for p in preds]
    return corpus_gleu(refs_tok, preds_tok) * 100

# ---------------------------
# DEVICE
# ---------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ---------------------------
# TRAINING LOOP WITH PROGRESS BAR
# ---------------------------
results = {}

for model_name in MODELS:
    try:
        print(f"\n=== TRAINING {model_name} ===")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.add_special_tokens({"pad_token": "[PAD]"})

        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
        model.resize_token_embeddings(len(tokenizer))

        train_ds = RecipeDS(train_df, tokenizer, MAX_IN, MAX_OUT)
        val_ds = RecipeDS(val_df, tokenizer, MAX_IN, MAX_OUT)
        test_ds = RecipeDS(test_df, tokenizer, MAX_IN, MAX_OUT)

        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)
        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)

        optim = AdamW(model.parameters(), lr=LR)
        steps = math.ceil(len(train_ds) / (BATCH_SIZE * ACCUM)) * EPOCHS
        sched = get_scheduler("linear", optimizer=optim, num_warmup_steps=0, num_training_steps=steps)

        scaler = torch.cuda.amp.GradScaler()
        best_score = -1
        global_step = 0

        for ep in range(EPOCHS):
            model.train()
            running_loss = 0

            # tqdm progress bar
            train_loader_tqdm = tqdm.tqdm(train_loader, desc=f"Epoch {ep+1}", leave=True)
            
            for i, batch in enumerate(train_loader_tqdm):
                batch = {k:v.to(device) for k,v in batch.items()}

                with torch.cuda.amp.autocast():
                    out = model(**batch)
                    loss = out.loss / ACCUM

                scaler.scale(loss).backward()
                running_loss += out.loss.detach().cpu().item()

                if (i+1) % ACCUM == 0:
                    scaler.step(optim)
                    scaler.update()
                    sched.step()
                    optim.zero_grad()
                    global_step += 1

                # update progress bar
                train_loader_tqdm.set_postfix({"loss": running_loss/(i+1)})

            # Validation
            model.eval()
            preds, refs = [], []
            with torch.no_grad():
                for b in val_loader:
                    b = {k:v.to(device) for k,v in b.items()}
                    gen = model.generate(input_ids=b["input_ids"], attention_mask=b["attention_mask"],
                                         max_length=MAX_OUT+8, num_beams=4)
                    
                    p = tokenizer.batch_decode(gen, skip_special_tokens=True)
                    lab = b["labels"].cpu().numpy()
                    lab[lab==-100] = tokenizer.pad_token_id
                    r = tokenizer.batch_decode(lab, skip_special_tokens=True)
                    
                    preds += p
                    refs += r

            score = rougeL(preds, refs)
            print(f"Epoch {ep+1} | Val ROUGE-L = {score:.2f}")

            if score > best_score:
                best_score = score
                model_dir = os.path.join(SAVE_DIR, model_name.replace("/", "_"))
                os.makedirs(model_dir, exist_ok=True)
                model.save_pretrained(model_dir)
                tokenizer.save_pretrained(model_dir)
                print(">>> Best model saved.")

        # Test evaluation
        print(f"--- Testing {model_name} on Test Set ---")
        try:
             model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(SAVE_DIR, model_name.replace("/", "_"))).to(device)
        except:
             print("Could not load best saved model. Using last epoch checkpoint.")
             
        model.eval()
        preds, refs = [], []
        with torch.no_grad():
            for b in test_loader:
                b = {k:v.to(device) for k,v in b.items()}
                gen = model.generate(input_ids=b["input_ids"], attention_mask=b["attention_mask"],
                                     max_length=MAX_OUT+8, num_beams=4)
                
                p = tokenizer.batch_decode(gen, skip_special_tokens=True)
                lab = b["labels"].cpu().numpy()
                lab[lab==-100] = tokenizer.pad_token_id
                r = tokenizer.batch_decode(lab, skip_special_tokens=True)
                preds += p
                refs += r

        results[model_name] = {
            "rougeL": rougeL(preds, refs),
            "bleu": bleu_score(preds, refs),
            "gleu": gleu_score(preds, refs)
        }
        print(f"=== {model_name} TEST SCORES ===", results[model_name])

        free()

    except Exception as e:
        results[model_name] = {"error": str(e)}
        print(f"Model {model_name} failed:", e)

print("\n=== ALL MODELS FINISHED ===")
print(results)

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":5245331,"sourceType":"datasetVersion","datasetId":3052101}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================\n# TPU-SAFE MULTI-MODEL SEQ2SEQ TRAINING\n# ============================\n\nimport os, gc, math, ast, re, time, warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# TPU\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler\nfrom torch.optim import AdamW\n\n# Evaluation\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.translate.gleu_score import corpus_gleu\n\n# ---------------------------\n# CONFIGURATION\n# ---------------------------\nDATA_PATH = \"/kaggle/input/recipenlg/dataset/full_dataset.csv\"\nMODELS = [\"t5-small\", \"t5-base\", \"facebook/bart-base\",\"distilbert-base-uncased\", \"roberta-base\"]\nFRAC = 0.1\nMAX_IN = 64\nMAX_OUT = 12\nBATCH_PER_CORE = 4\nACCUM = 2\nEPOCHS = 3\nLR = 3e-4\nSAVE_DIR = \"./best_tpu_models\"\n\n# ---------------------------\n# FREE MEMORY\n# ---------------------------\ndef free():\n    gc.collect()\n    try: torch.cuda.empty_cache()\n    except: pass\n\n# ---------------------------\n# LOAD CSV + CLEAN\n# ---------------------------\ndf = pd.read_csv(DATA_PATH)\ndf.columns = [c.lower().strip() for c in df.columns]\ndf = df.dropna(subset=[\"title\", \"ner\"])\n\ndef parse(x):\n    try:\n        arr = ast.literal_eval(x)\n        arr = [str(a).lower().strip() for a in arr]\n        return \", \".join(sorted(list(set(arr))))\n    except:\n        return str(x).lower()\n\ndef clean(t):\n    t = str(t).lower()\n    t = re.sub(r\"[^a-z0-9 ]\", \" \", t)\n    return re.sub(r\"\\s+\", \" \", t).strip()\n\ndf[\"ingredients\"] = df[\"ner\"].apply(parse)\ndf[\"title\"] = df[\"title\"].apply(clean)\ndf = df[df[\"title\"].str.len() > 5]\ndf = df[df[\"ingredients\"].str.len() > 5]\ndf = df.sample(frac=FRAC, random_state=42).reset_index(drop=True)\n\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=1)\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=1)\n\n# ---------------------------\n# DATASET\n# ---------------------------\nclass RecipeDS(Dataset):\n    def __init__(self, df, tokenizer, max_in=64, max_out=12):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_in = max_in\n        self.max_out = max_out\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        src = \"generate title: \" + row[\"ingredients\"]\n        tgt = row[\"title\"]\n\n        x = self.tokenizer(src, max_length=self.max_in, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n        y = self.tokenizer(tgt, max_length=self.max_out, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n\n        labels = y[\"input_ids\"].squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": x[\"input_ids\"].squeeze().long(),\n            \"attention_mask\": x[\"attention_mask\"].squeeze().long(),\n            \"labels\": labels.long()\n        }\n\n# ---------------------------\n# TPU DEVICE\n# ---------------------------\ndevice = xm.xla_device()\nworld = 1  # single TPU core (for Colab TPU VM)\nrank = 0\nmaster = True\n\n# ---------------------------\n# EVALUATION FUNCTIONS\n# ---------------------------\nscorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\ndef rougeL(preds, refs):\n    scores = [scorer.score(a, b)[\"rougeL\"].fmeasure for a, b in zip(preds, refs)]\n    return float(np.mean(scores)) * 100\n\ndef bleu_score(preds, refs):\n    refs_tok = [[r.split()] for r in refs]\n    preds_tok = [p.split() for p in preds]\n    return corpus_bleu(refs_tok, preds_tok) * 100\n\ndef gleu_score(preds, refs):\n    refs_tok = [[r.split()] for r in refs]\n    preds_tok = [p.split() for p in preds]\n    return corpus_gleu(refs_tok, preds_tok) * 100\n\n# ---------------------------\n# TRAINING PIPELINE\n# ---------------------------\nresults = {}\n\nfor model_name in MODELS:\n    print(f\"\\n=== TRAINING {model_name} ===\")\n    try:\n        tok = AutoTokenizer.from_pretrained(model_name)\n        if tok.pad_token is None:\n            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n        model.resize_token_embeddings(len(tok))\n\n        train_ds = RecipeDS(train_df, tok, MAX_IN, MAX_OUT)\n        val_ds = RecipeDS(val_df, tok, MAX_IN, MAX_OUT)\n        test_ds = RecipeDS(test_df, tok, MAX_IN, MAX_OUT)\n\n        train_loader = DataLoader(train_ds, batch_size=BATCH_PER_CORE, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_PER_CORE)\n        test_loader = DataLoader(test_ds, batch_size=BATCH_PER_CORE)\n\n        optim = AdamW(model.parameters(), lr=LR)\n        steps = math.ceil(len(train_ds) / (BATCH_PER_CORE * ACCUM)) * EPOCHS\n        sched = get_scheduler(\"linear\", optimizer=optim, num_warmup_steps=0, num_training_steps=steps)\n\n        best_score = -1\n        global_step = 0\n\n        for ep in range(EPOCHS):\n            model.train()\n            running_loss = 0\n\n            for i, batch in enumerate(train_loader):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                out = model(**batch)\n                loss = out.loss / ACCUM\n                loss.backward()\n                running_loss += out.loss.detach().cpu().item()\n\n                if (i + 1) % ACCUM == 0:\n                    optim.step()\n                    sched.step()\n                    optim.zero_grad()\n                    global_step += 1\n\n                if (i + 1) % (20 * ACCUM) == 0 and master:\n                    print(f\"Epoch {ep+1} Step {i+1} Loss {running_loss / (i+1):.4f}\")\n\n            # ---------------------------\n            # Validation\n            # ---------------------------\n            model.eval()\n            if master:\n                preds, refs = [], []\n                for b in val_loader:\n                    with torch.no_grad():\n                        b = {k: v.to(device) for k, v in b.items()}\n                        gen = model.generate(\n                            b[\"input_ids\"],\n                            attention_mask=b[\"attention_mask\"],\n                            max_length=MAX_OUT + 8,\n                            num_beams=4\n                        )\n                    p = tok.batch_decode(gen.cpu(), skip_special_tokens=True)\n                    lab = b[\"labels\"].cpu().numpy()\n                    lab[lab == -100] = tok.pad_token_id\n                    r = tok.batch_decode(lab, skip_special_tokens=True)\n                    preds += p\n                    refs += r\n\n                score = rougeL(preds, refs)\n                print(f\"Epoch {ep+1} | Val ROUGE-L = {score:.2f}\")\n\n                if score > best_score:\n                    best_score = score\n                    os.makedirs(f\"{SAVE_DIR}/{model_name.replace('/', '_')}\", exist_ok=True)\n                    model.save_pretrained(f\"{SAVE_DIR}/{model_name.replace('/', '_')}\")\n                    tok.save_pretrained(f\"{SAVE_DIR}/{model_name.replace('/', '_')}\")\n                    print(\">>> Best model saved.\")\n\n        # ---------------------------\n        # TEST\n        # ---------------------------\n        if master:\n            print(f\"Testing best model {model_name}...\")\n            model.eval()\n            preds, refs = [], []\n\n            for b in test_loader:\n                with torch.no_grad():\n                    b = {k: v.to(device) for k, v in b.items()}\n                    gen = model.generate(\n                        b[\"input_ids\"],\n                        attention_mask=b[\"attention_mask\"],\n                        max_length=MAX_OUT + 8,\n                        num_beams=4\n                    )\n                p = tok.batch_decode(gen.cpu(), skip_special_tokens=True)\n                lab = b[\"labels\"].cpu().numpy()\n                lab[lab == -100] = tok.pad_token_id\n                r = tok.batch_decode(lab, skip_special_tokens=True)\n                preds += p\n                refs += r\n\n            results[model_name] = {\n                \"rougeL\": rougeL(preds, refs),\n                \"bleu\": bleu_score(preds, refs),\n                \"gleu\": gleu_score(preds, refs)\n            }\n            print(f\"FINAL TEST SCORES for {model_name}: {results[model_name]}\")\n\n    except Exception as e:\n        print(f\"Model {model_name} failed:\", str(e))\n        results[model_name] = {\"error\": str(e)}\n\nprint(\"\\n=== ALL MODELS FINISHED ===\")\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:59:30.048537Z","iopub.execute_input":"2025-11-26T08:59:30.048834Z"}},"outputs":[{"name":"stdout","text":"\n=== TRAINING t5-small ===\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
!pip install nltk
!pip install evaluate
!pip install rouge_score

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:128,expandable_segments:True")
import gc
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
# ---- Standard imports ----
import pandas as pd, ast, re
import numpy as np
import nltk
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    EncoderDecoderModel,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
import evaluate
from tqdm import tqdm

nltk.download('punkt', quiet=True)

# ---- Runtime detection (Gpu/TPU) ----
USE_CUDA = torch.cuda.is_available()
USE_MPS = hasattr(torch.backends, "mps") and torch.backends.mps.is_available()

IS_XLA = False
try:
    import torch_xla.core.xla_model as xm  # type: ignore
    IS_XLA = True
except Exception:
    IS_XLA = False

print(f"Runtime detect => CUDA={USE_CUDA}, MPS={USE_MPS}, XLA(TPU)={IS_XLA}")

# Mixed precision choices
use_fp16 = False
use_bf16 = False
if IS_XLA:
    use_bf16 = True   # TPUs prefer bfloat16
elif USE_CUDA or USE_MPS:
    use_fp16 = True   # GPUs/MPS use fp16 if available

print(f"Mixed precision: fp16={use_fp16}, bf16={use_bf16}")

# ---- Utility: free GPU memory ----
def free_memory():
    gc.collect()
    try:
        torch.cuda.empty_cache()
        if IS_XLA:
            xm.rendezvous("noop")  # small sync if XLA available
    except Exception:
        pass

# ---- Configurable hyperparameters ----
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

DATA_PATH = "/kaggle/input/recipenlg/dataset/full_dataset.csv"  # update if needed
SAMPLE_FRAC = 0.10   # change to 1.0 for full dataset
MAX_INPUT_LENGTH = 64
MAX_TARGET_LENGTH = 12

PER_DEVICE_TRAIN_BATCH = 2     # small -> GPU safe
PER_DEVICE_EVAL_BATCH = 2
GRADIENT_ACCUMULATION_STEPS = 4
NUM_EPOCHS = 3
LEARNING_RATE = 5e-4
EARLY_STOPPING_PATIENCE = 2

# ---- Load & preprocess dataset ----
df = pd.read_csv(DATA_PATH)
df.columns = [c.lower().strip() for c in df.columns]

title_col = "title"
ner_col = "ner"
df = df.dropna(subset=[title_col, ner_col])

def parse_ner_entities(text):
    try:
        if isinstance(text, str) and text.startswith("[") and text.endswith("]"):
            items = ast.literal_eval(text)
            items = [str(i).lower().strip() for i in items if str(i).strip()]
            items = sorted(list(set(items)))
            return ", ".join(items)
        return str(text).lower()
    except:
        return str(text).lower()

def simple_clean_text(text):
    text = str(text).lower()
    text = re.sub(r"[^a-zA-Z0-9 ]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df[ner_col] = df[ner_col].apply(parse_ner_entities)
df[title_col] = df[title_col].apply(simple_clean_text)

df = df[df['title'].str.len() > 5]
df = df[df['ner'].str.len() > 10]
df = df[[ner_col, title_col]].drop_duplicates()

df = df.rename(columns={ner_col: "ingredients", title_col: "title"})
df = df.sample(frac=SAMPLE_FRAC, random_state=SEED).reset_index(drop=True)

train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=SEED)

dataset = DatasetDict({
    "train": Dataset.from_pandas(train_df.reset_index(drop=True)),
    "validation": Dataset.from_pandas(val_df.reset_index(drop=True)),
    "test": Dataset.from_pandas(test_df.reset_index(drop=True))
})

# ---- Models to try (edit list) ----
models_to_try = {
    "t5-base": "seq2seq",
    "facebook/bart-base": "seq2seq",
    "distilbert-base-uncased": "encoder-decoder", 
    "roberta-base": "encoder-decoder"
}

# ---- Load tokenizers ----
tokenizers = {}
for model_name in models_to_try:
    print("Loading tokenizer:", model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_remote_code=True)
    # add pad/cls/sep if missing for encoder-only
    if tokenizer.pad_token is None:
        if any(x in model_name for x in ["bert", "distilbert", "roberta"]):
            tokenizer.add_special_tokens({"pad_token": "[PAD]", "cls_token": "[CLS]", "sep_token": "[SEP]"})
        else:
            tokenizer.add_special_tokens({"pad_token": "[PAD]"})
    tokenizers[model_name] = tokenizer

# ---- Tokenization function ----
def tokenize_batch(example, tokenizer):
    inputs = ["title generation: " + x if 't5' in tokenizer.name_or_path else x for x in example["ingredients"]]
    targets = example["title"]
    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, padding="max_length", truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, padding="max_length", truncation=True)["input_ids"]
    model_inputs["labels"] = labels
    return model_inputs

prepared_datasets = {}
for model_name, mtype in models_to_try.items():
    print("Tokenizing dataset for:", model_name)
    tok = tokenizers[model_name]
    tokenized = dataset.map(lambda x: tokenize_batch(x, tok), batched=True, remove_columns=["title", "ingredients"])
    prepared_datasets[model_name] = tokenized

# ---- Metric setup ----
metric = evaluate.load("rouge")
def compute_metrics(eval_pred, tokenizer):
    preds, labels = eval_pred
    if isinstance(preds, tuple):
        preds = preds[0]
    # replace -100 with pad_token_id for decoding
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(nltk.sent_tokenize(p.strip())) for p in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(l.strip())) for l in decoded_labels]
    res = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    return {k: round(v * 100, 4) for k, v in res.items()}

# ---- Train & evaluate function (GPU/TPU/CPU aware) ----
def train_and_evaluate(model_name, dataset_dict, tokenizer, model_type):
    print(f"\n=== Train start: {model_name} ===")
    free_memory()

    # Build model
    if model_type == "seq2seq":
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    else:
        model = EncoderDecoderModel.from_encoder_decoder_pretrained(model_name, model_name)
        model.config.decoder.vocab_size = len(tokenizer)
        model.config.decoder_start_token_id = tokenizer.cls_token_id or tokenizer.bos_token_id
        model.config.pad_token_id = tokenizer.pad_token_id
        model.config.eos_token_id = tokenizer.sep_token_id or tokenizer.eos_token_id
        model.config.use_cache = False

    # resize embeddings if tokenizer changed
    if tokenizer.pad_token is not None and getattr(model.config, "vocab_size", None) != len(tokenizer):
        model.resize_token_embeddings(len(tokenizer))

    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

    # TrainingArguments (safe defaults)
    output_dir = f"./outputs/{model_name.replace('/', '_')}"
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,
        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        fp16=use_fp16,
        bf16=use_bf16,
        optim="adamw_torch",
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="steps",
        logging_steps=100,
        report_to="none",
        dataloader_num_workers=0,          
        eval_accumulation_steps=1,        
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        save_total_limit=2,
        run_name=f"run_{model_name.replace('/', '_')}"
    )

    callbacks = [EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)]

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset_dict["train"],
        eval_dataset=dataset_dict["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda p: compute_metrics(p, tokenizer),
        callbacks=callbacks
    )

    # Train with OOM fallback strategy
    try:
        trainer.train()
    except RuntimeError as e:
        msg = str(e).lower()
        print("Trainer.train() exception:", msg)
        if "out of memory" in msg or "cuda out of memory" in msg:
            print("OOM during training â€” applying fallback: reduce batch size, disable fp16/bf16, increase grad_accum.")
            # modify trainer args for fallback
            trainer.args.per_device_train_batch_size = max(1, trainer.args.per_device_train_batch_size // 2)
            trainer.args.per_device_eval_batch_size = max(1, trainer.args.per_device_eval_batch_size // 2)
            trainer.args.gradient_accumulation_steps = max(1, trainer.args.gradient_accumulation_steps * 2)
            trainer.args.fp16 = False
            trainer.args.bf16 = False
            free_memory()
            trainer.train()
        else:
            raise

    # Evaluate on test set (safe eval accumulation)
    test_metrics = trainer.evaluate(dataset_dict["test"])
    print(f"Test metrics for {model_name}: {test_metrics}")

    # Cleanup & free memory
    try:
        del model, trainer
        free_memory()
    except Exception:
        pass

    return test_metrics

# ---- Run models sequentially (safe) ----
all_metrics = {}
for model_name, model_type in models_to_try.items():
    print(f"\nPreparing model: {model_name}")
    tok = tokenizers[model_name]
    ds = prepared_datasets[model_name]
    try:
        m = train_and_evaluate(model_name, ds, tok, model_type)
        all_metrics[model_name] = m
    except Exception as e:
        print(f"Model {model_name} failed: {e}")
        all_metrics[model_name] = {"error": str(e)}
    free_memory()

print("\n=== ALL MODELS FINISHED ===")
print(all_metrics)

# ---- Optional: sample generation function (for seq2seq models) ----
def generate_examples(model, tokenizer, inputs, max_length=32, device=None):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    enc = tokenizer(inputs, return_tensors="pt", padding=True, truncation=True, max_length=MAX_INPUT_LENGTH)
    for k, v in enc.items():
        enc[k] = v.to(device)
    gen = model.generate(**enc, max_length=max_length, num_beams=4, early_stopping=True)
    return tokenizer.batch_decode(gen, skip_special_tokens=True)

# Done.
